{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv9 Detection + Yaw Estimation + Kalman Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = 'video8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv9 + Yaw Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting using GPU...\n",
      "Frame 0\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 1 truck, 374.0ms\n",
      "Speed: 5.6ms preprocess, 374.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 1\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 1 truck, 372.2ms\n",
      "Speed: 2.8ms preprocess, 372.2ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 2\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 1 truck, 372.8ms\n",
      "Speed: 3.0ms preprocess, 372.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 3\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 1 truck, 371.3ms\n",
      "Speed: 3.5ms preprocess, 371.3ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 4\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 1 truck, 372.6ms\n",
      "Speed: 4.0ms preprocess, 372.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 5\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 1 truck, 372.4ms\n",
      "Speed: 4.0ms preprocess, 372.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 6\n",
      "\n",
      "0: 384x640 1 person, 4 cars, 1 train, 1 truck, 370.9ms\n",
      "Speed: 3.5ms preprocess, 370.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 7\n",
      "\n",
      "0: 384x640 1 person, 3 cars, 1 train, 1 truck, 371.6ms\n",
      "Speed: 3.0ms preprocess, 371.6ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 8\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 1 truck, 373.2ms\n",
      "Speed: 2.5ms preprocess, 373.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 9\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 1 train, 1 truck, 371.8ms\n",
      "Speed: 3.0ms preprocess, 371.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 10\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 1 truck, 371.0ms\n",
      "Speed: 4.0ms preprocess, 371.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 11\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 1 truck, 372.7ms\n",
      "Speed: 3.0ms preprocess, 372.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 12\n",
      "\n",
      "0: 384x640 6 cars, 1 train, 1 truck, 370.5ms\n",
      "Speed: 3.0ms preprocess, 370.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 13\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 1 train, 1 truck, 370.8ms\n",
      "Speed: 3.5ms preprocess, 370.8ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 14\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 1 truck, 371.1ms\n",
      "Speed: 4.1ms preprocess, 371.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 15\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 1 truck, 373.5ms\n",
      "Speed: 3.5ms preprocess, 373.5ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 16\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 371.1ms\n",
      "Speed: 4.0ms preprocess, 371.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 17\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 375.3ms\n",
      "Speed: 3.5ms preprocess, 375.3ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 18\n",
      "\n",
      "0: 384x640 3 cars, 1 train, 371.3ms\n",
      "Speed: 3.5ms preprocess, 371.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 19\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 371.4ms\n",
      "Speed: 3.0ms preprocess, 371.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 20\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 372.6ms\n",
      "Speed: 3.5ms preprocess, 372.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 21\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 371.2ms\n",
      "Speed: 3.0ms preprocess, 371.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 22\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 372.1ms\n",
      "Speed: 2.9ms preprocess, 372.1ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 23\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 371.8ms\n",
      "Speed: 4.0ms preprocess, 371.8ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 24\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 371.0ms\n",
      "Speed: 5.0ms preprocess, 371.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 25\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 372.0ms\n",
      "Speed: 3.1ms preprocess, 372.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 26\n",
      "\n",
      "0: 384x640 6 cars, 1 train, 371.3ms\n",
      "Speed: 4.2ms preprocess, 371.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 27\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 371.7ms\n",
      "Speed: 3.0ms preprocess, 371.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 28\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 373.0ms\n",
      "Speed: 3.6ms preprocess, 373.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 29\n",
      "\n",
      "0: 384x640 6 cars, 1 train, 372.7ms\n",
      "Speed: 3.1ms preprocess, 372.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 30\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 370.9ms\n",
      "Speed: 3.0ms preprocess, 370.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 31\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 371.1ms\n",
      "Speed: 3.0ms preprocess, 371.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 32\n",
      "\n",
      "0: 384x640 6 cars, 1 train, 371.7ms\n",
      "Speed: 4.1ms preprocess, 371.7ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 33\n",
      "\n",
      "0: 384x640 6 cars, 1 train, 370.9ms\n",
      "Speed: 3.5ms preprocess, 370.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 34\n",
      "\n",
      "0: 384x640 7 cars, 1 train, 370.3ms\n",
      "Speed: 4.0ms preprocess, 370.3ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 35\n",
      "\n",
      "0: 384x640 7 cars, 1 train, 371.9ms\n",
      "Speed: 4.0ms preprocess, 371.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 36\n",
      "\n",
      "0: 384x640 6 cars, 1 train, 371.0ms\n",
      "Speed: 4.0ms preprocess, 371.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 37\n",
      "\n",
      "0: 384x640 7 cars, 1 train, 371.5ms\n",
      "Speed: 3.0ms preprocess, 371.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 38\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 371.4ms\n",
      "Speed: 3.0ms preprocess, 371.4ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 39\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 372.5ms\n",
      "Speed: 3.0ms preprocess, 372.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 40\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 371.1ms\n",
      "Speed: 7.2ms preprocess, 371.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 41\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 372.3ms\n",
      "Speed: 3.0ms preprocess, 372.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 42\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 371.0ms\n",
      "Speed: 4.0ms preprocess, 371.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 43\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 372.7ms\n",
      "Speed: 5.0ms preprocess, 372.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 44\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 371.2ms\n",
      "Speed: 3.4ms preprocess, 371.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 45\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 370.8ms\n",
      "Speed: 4.5ms preprocess, 370.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 46\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 372.0ms\n",
      "Speed: 4.0ms preprocess, 372.0ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 47\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 371.4ms\n",
      "Speed: 3.0ms preprocess, 371.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 48\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 370.0ms\n",
      "Speed: 4.0ms preprocess, 370.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 49\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 371.5ms\n",
      "Speed: 3.6ms preprocess, 371.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 50\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 370.4ms\n",
      "Speed: 4.0ms preprocess, 370.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 51\n",
      "\n",
      "0: 384x640 4 cars, 1 bus, 1 train, 371.5ms\n",
      "Speed: 3.5ms preprocess, 371.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 52\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 370.5ms\n",
      "Speed: 4.5ms preprocess, 370.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 53\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 369.9ms\n",
      "Speed: 5.0ms preprocess, 369.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 54\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 370.4ms\n",
      "Speed: 3.0ms preprocess, 370.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 55\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 371.0ms\n",
      "Speed: 4.0ms preprocess, 371.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 56\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 370.9ms\n",
      "Speed: 4.1ms preprocess, 370.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 57\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 371.4ms\n",
      "Speed: 4.0ms preprocess, 371.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 58\n",
      "\n",
      "0: 384x640 5 cars, 1 train, 371.0ms\n",
      "Speed: 3.2ms preprocess, 371.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 59\n",
      "\n",
      "0: 384x640 4 cars, 1 train, 372.0ms\n",
      "Speed: 3.0ms preprocess, 372.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from deepbox.deepbox import Deepbox\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv9\n",
    "device_name = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)\n",
    "    device_name = '0'\n",
    "    yolov9 = YOLO(\"yolov9c.pt\").to('cuda')\n",
    "    print('Detecting using GPU...')\n",
    "else:\n",
    "    device_name = 'cpu'\n",
    "    yolov9 = YOLO(\"yolov9c.pt\").to('cpu')\n",
    "    print('Detecting using CPU...')\n",
    "\n",
    "# Load Deepbox\n",
    "deepbox = Deepbox()\n",
    "\n",
    "# Read video file\n",
    "input_file_name = f'video/{video_name}.mp4'\n",
    "cap = cv2.VideoCapture(input_file_name)\n",
    "\n",
    "# get height, width and frame count of the video\n",
    "width, height = (\n",
    "        int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    )\n",
    "no_of_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "proc_frames = 0\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "out = cv2.VideoWriter()\n",
    "output_file_name = f\"output/{video_name}_yolov9.mp4\"\n",
    "out.open(output_file_name, fourcc, fps, (width, height), True)\n",
    "\n",
    "frames = []\n",
    "frames_detections = []\n",
    "try:\n",
    "    for f in range(no_of_frames):\n",
    "        print(f'Frame {f}')\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # yolov9 detection\n",
    "        result = yolov9.predict(frame)[0]\n",
    "        frame_detections = []\n",
    "        for i, box in enumerate(result.boxes):\n",
    "            # accepted_class_names = ['car', 'bus', 'truck']\n",
    "            accepted_class_names = ['car']\n",
    "            class_index = int(result.boxes.cls[i])\n",
    "            class_name = result.names[class_index]\n",
    "            conf = float(result.boxes.conf[i])\n",
    "            xywh = result.boxes.xywh[i]\n",
    "\n",
    "            x1 = int(xywh[0] - xywh[2]/2)\n",
    "            y1 = int(xywh[1] - xywh[3]/2)\n",
    "            x2 = int(xywh[0] + xywh[2]/2)\n",
    "            y2 = int(xywh[1] + xywh[3]/2)\n",
    "        \n",
    "            det = xywh.tolist() +  [class_name, conf]\n",
    "            if class_name not in accepted_class_names:\n",
    "                continue\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "            cv2.putText(frame, f'{det[4]} ({det[5]:.2f})', (int(det[0]), int(det[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            frame_detections.append(det)\n",
    "        \n",
    "        # yaw estimation\n",
    "        detc_2ds = []\n",
    "        for det in frame_detections:\n",
    "            # class_name = result.names[det[4]]\n",
    "            class_name = det[4]\n",
    "            xywh = det\n",
    "            x1 = int(xywh[0] - xywh[2]/2)\n",
    "            y1 = int(xywh[1] - xywh[3]/2)\n",
    "            x2 = int(xywh[0] + xywh[2]/2)\n",
    "            y2 = int(xywh[1] + xywh[3]/2)\n",
    "            detc_2ds.append([class_name, [x1, y1, x2, y2]])\n",
    "        yaws = deepbox.predict(frame, detc_2ds)\n",
    "        yaw_degrees = [np.rad2deg(yaw) for yaw in yaws]\n",
    "\n",
    "        # convert to frame detections [x, y, w, h, yaw, class_index, conf]\n",
    "        frame_detections = [[det[0], det[1], det[2], det[3], yaw_degrees[i], det[4], det[5]] for i, det in enumerate(frame_detections)]\n",
    "\n",
    "        out.write(frame)\n",
    "        frames_detections.append(frame_detections)\n",
    "\n",
    "    # Add to frames detections\n",
    "    with open(f'output/{video_name}_yolov9.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump({ \"final_frames_detections\": frames_detections }, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "except Exception:\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "finally:\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!ffmpeg -i video/video8.mp4 -f mp3 -ab 192000 -vn output/video8_audio.mp3 -y\n",
    "!ffmpeg -i output/video8_yolov9.mp4 -i output/video8_audio.mp3 -c:v libx264 -c:a copy -map 0:v:0 -map 1:a:0 output/video8_yolov9_audio.mp4 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sort.vehicle_tracking import SortVehicle\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "input_file_name = f\"video/{video_name}.mp4\"\n",
    "output_file_name = f\"output/{video_name}_tracked.mp4\"\n",
    "output_state_file_name = f\"output/{video_name}_states.json\"\n",
    "\n",
    "sort = SortVehicle(max_age=15, min_hits=3, iou_threshold=0.3, zc=640, offset_x=0)\n",
    "\n",
    "cap = cv2.VideoCapture(input_file_name)\n",
    "with open(f'output/{video_name}_yolov9.json') as f:\n",
    "    detections = json.load(f)['final_frames_detections']\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width, height = (\n",
    "            int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "            int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    ")\n",
    "out = cv2.VideoWriter()\n",
    "out.open(output_file_name, fourcc, fps, (width, height), True)\n",
    "\n",
    "proc_frames = 0\n",
    "state_rotations = []\n",
    "color_mapping = {}\n",
    "try:\n",
    "    while proc_frames < len(detections):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        im = frame\n",
    "        # Loop through list (if empty this will be skipped) and overlay green bboxes\n",
    "        frame_detections = detections[proc_frames]\n",
    "        frame_detections_converted = []\n",
    "        for Z in frame_detections:\n",
    "            frame_detections_converted.append([Z[0]-Z[2]/2, Z[1]-Z[3]/2, Z[0]+Z[2]/2, Z[1]+Z[3]/2, Z[4]])\n",
    "        frame_detections_converted = np.array(frame_detections_converted)\n",
    "\n",
    "        # tracking\n",
    "        if len(frame_detections_converted) > 0:\n",
    "            Zs, Xs = sort.update(frame_detections_converted)\n",
    "        else:\n",
    "            Zs, Xs = sort.update(np.empty((0, 5)))\n",
    "\n",
    "        temp = []\n",
    "        for (Z, X) in zip(Zs, Xs):\n",
    "            vehicle_id = int(Z[-1])\n",
    "            if vehicle_id not in color_mapping:\n",
    "                bounding_rect = im[int(Z[1]):int(Z[3]), int(Z[0]):int(Z[2])]\n",
    "                try:\n",
    "                    bgr_color = np.average(np.average(bounding_rect, axis=0), axis=0)\n",
    "                except Exception as error:\n",
    "                    continue\n",
    "\n",
    "                hsv_color = cv2.cvtColor(np.array([[bgr_color]]).astype(np.uint8), cv2.COLOR_BGR2HSV)\n",
    "                hsv_color[:, :, 1] = 200\n",
    "                bgr_color = cv2.cvtColor(hsv_color, cv2.COLOR_HSV2BGR)[0, 0]\n",
    "\n",
    "                rgb_color = [int(bgr_color[2]), int(bgr_color[1]), int(bgr_color[0])]\n",
    "                color_mapping[vehicle_id] = rgb_color\n",
    "\n",
    "            temp.append(X.tolist() + [Z[0], Z[1], Z[2], Z[3], int(Z[-1]), color_mapping[vehicle_id]])\n",
    "            cv2.putText(im, f'{int(Z[6])} ({int(X[0])},0,{int(X[2])},{int(X[3])})', (int((Z[0] + Z[2])//2), int((Z[1] + Z[3])//2)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "            cv2.rectangle(im, (int(Z[0]), int(Z[1])), (int(Z[2]), int(Z[3])), (0, 255, 0), 3)\n",
    "        state_rotations.append(temp)\n",
    "\n",
    "        # write the frame\n",
    "        out.write(im)\n",
    "\n",
    "        proc_frames += 1\n",
    "except Exception as error:\n",
    "    # Release resources\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "finally:\n",
    "    out.release()\n",
    "    cap.release()\n",
    "\n",
    "with open(output_state_file_name, 'w', encoding='utf-8') as f:\n",
    "    json.dump({ \"states\": state_rotations }, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!ffmpeg -i output/video8_tracked.mp4 -i output/video8_audio.mp3 -c:v libx264 -c:a copy -map 0:v:0 -map 1:a:0 output/video8_tracked_audio.mp4 -y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
